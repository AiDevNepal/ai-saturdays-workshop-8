{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "It is one of the simplest machine learning model for text classification. It uses the probabilisti distribution of tokens/words (counts) to classify documents. It is based on infamous **Bayes Theorem** which goes like this:\n",
    "\n",
    "`Prob(B | A) = Prob(A | B) * Prob(B) / Prob(A)`\n",
    "\n",
    "Fair enough, right?\n",
    "\n",
    "## Text Classification\n",
    "Say we have Document **D** that belongs to class **C**. So, using bayes theorem we can infer:  \n",
    "\n",
    "`Prob(C | D) = Prob(D | C) * Prob(C) / Prob(D)`\n",
    "\n",
    "So far so good.  \n",
    "\n",
    "We know a document is made up of tokens (combination of tokens, commonly referred to as **ngram language model**):  \n",
    "`D = [d1, d2, d3, ...]`\n",
    "\n",
    "```bash\n",
    "Prob(C | D)\n",
    "= Prob(d1 | C) * Prob(d2 | C) * Prob(d3 | C) .... * Prob(C) / Prob(D)\n",
    "```\n",
    "\n",
    "Remember, we have seggregated Prob(D | C) to individual probabilities of individual tokens (ngrams) constituting\n",
    "document D. This is why Naive Bayes classifier is **Naive** - it assumes  each tokens are independent of each other.  \n",
    "  \n",
    "Think it of as two independent events **A** and **B**. So, what's the probability of both events occuring simultaneously?  \n",
    "`Prob(A, B) = Prob(A) * Prob(B)`\n",
    "\n",
    "Now we can infer the Probabities Prob(di | C) as :  \n",
    "` (count(di) that belongs to class C) / (total number of tokens)`\n",
    "\n",
    "### Putting Things Into Perspective\n",
    "And that is how we can find the probability of document **D** beloning to class **C** assuming independence \n",
    "of individual features(ngrams). \n",
    "\n",
    "Now, say we have classes:  \n",
    "C1, C2, C3, ...  \n",
    "\n",
    "\n",
    "And we want to classify a test document **D**. All we have to do is find the probabilty of this document **D**\n",
    "beloning to each of the classes. And we choose the class where **Prob(D | C)** is the highest.\n",
    "\n",
    "#### Training Steps (somewhat)\n",
    "It's nothing but counting the \"stuff\" that matter.\n",
    "- tokenize the documents for each classes\n",
    "- each token can be unigram, bigram, ...\n",
    "- extract features for each token -> counts or tf-idf\n",
    "\n",
    "#### Let's classify\n",
    "- extract features (count) for the document to be classified\n",
    "- calculate **Prob(C1 | D)**\n",
    "- Calculate **Prob(C2 | D)**\n",
    "- Calculate **Prob(C3 | D)**\n",
    "- choose the Class **Ci** that has max probability\n",
    "\n",
    "**Side note**:  \n",
    "Since **Prob(D)** is constant, we can ignore the denominator part and just focus on the numerator's products.  \n",
    "\n",
    "So, all we are doing is:  \n",
    "\n",
    "Choose class **Ci** according to argmax{ Prob(Ci | D) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noob documents for training :P\n",
    "spam = [\n",
    "    \"you have won a lottery\",\n",
    "    \"congratulations! you have a bonus\",\n",
    "    \"this is bomb\",\n",
    "    \"to use the credit, please click the link\",\n",
    "    \"thank you for subscription. please click the link\",\n",
    "    \"bomb\"\n",
    "]\n",
    "Y_spam = [1 for i in range(len(spam)) ]\n",
    "\n",
    "non_spam = [\n",
    "    \"i am awesome\",\n",
    "    \"i have a meeting tomorrow\",\n",
    "    \"you are smart\",\n",
    "    \"get me out of here\",\n",
    "    \"call me later\"\n",
    "]\n",
    "Y_non_spam = [0 for i in range(len(non_spam)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2)).fit(spam + non_spam)\n",
    "X_train_vectorized = count_vectorizer.transform(spam + non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Model\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, Y_spam + Y_non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"call you\",\n",
    "    \"you have won\"\n",
    "]\n",
    "predictions = model.predict(count_vectorizer.transform(documents))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you have won a lottery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>congratulations! you have a bonus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is bomb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to use the credit, please click the link</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you for subscription. please click the link</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bomb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i am awesome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i have a meeting tomorrow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>you are smart</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get me out of here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>call me later</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target\n",
       "0                              you have won a lottery       1\n",
       "1                   congratulations! you have a bonus       1\n",
       "2                                        this is bomb       1\n",
       "3            to use the credit, please click the link       1\n",
       "4   thank you for subscription. please click the link       1\n",
       "5                                                bomb       1\n",
       "6                                        i am awesome       0\n",
       "7                           i have a meeting tomorrow       0\n",
       "8                                       you are smart       0\n",
       "9                                  get me out of here       0\n",
       "10                                      call me later       0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pandas dataframe for seamless training\n",
    "spam_df = pd.DataFrame(spam, columns=['text'])\n",
    "spam_df['target'] = 1\n",
    "non_spam_df = pd.DataFrame(non_spam, columns=['text'])\n",
    "non_spam_df['target'] = 0\n",
    "\n",
    "# final data\n",
    "data = pd.concat([spam_df, non_spam_df], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
